# 4th Semester Summer Internship ðŸš€

Excited to embark on summer internship at Kevalam Solutions as a Data Analyst and Machine Learning Intern!  This is a great opportunity to gain valuable experience in the field.

I'm super excited to share my experience as a Data Analyst and Machine Learning Intern at Kevalam Solutions for the past few weeks. I've been working on some really interesting projects with Slyce (Slyce Agri World), Gujarat's first online gourmet store, and I wanted to document my learnings so far.

-> Week 1: Diving into Slyce's Data

    Day 1: Started off by setting up my environment - got MySQL installed and familiarized myself with Slyce's database.
    Day 2: Looker Studio became my new best friend! Learned about this awesome data visualization tool to bring insights to life.
    Day 3: From theory to practice, I started querying the database to understand Slyce's customers better.
    Day 4: My first report! Visualized the distribution of Slyce's customers across different areas.
    Day 5: Identified Slyce's top-selling products and created a cool dashboard to display this information.
    Day 6: Analyzed data on new customer acquisition to see how Slyce's customer base is growing.


-> Week 2: Deliveries, Revenue, and Customer Behavior

    Day 7: Focused on delivery efficiency - analyzed the average delivery time per delivery person.
    Day 8: Dived deeper into customer behavior by calculating and analyzing average revenue per user.
    Day 9: Leveled up my data skills by extracting data from an Excel sheet and integrating it into the database.
    Day 10: WhatsApp and Facebook orders - analyzed data to understand how customers are using these platforms.
    Day 11: Following the money trail - analyzed the total revenue generated from WhatsApp and Facebook orders.
    Day 12: Investigated the link between customer subscriptions and their purchase behavior.

-> Week 3: Deep Dive into Customer Insights

    Day 13: Analyzed the total number of deliveries across different time periods to see trends.
    Day 14: Coupons are a great way to attract customers! Analyzed the value of coupons used by Slyce's customers.
    Day 15: Segmented customers into unique, repeat, and total customer groups to understand their buying patterns.
    Day 16: Who's done highest deliveries? Identified the delivery person with the highest number of deliveries.
    Day 17: Data transformation, Transformed data for year-wise and quarterly analysis in an Excel sheet.
    Day 18: Polished the design of my reports and dashboards to give them a professional look and feel.


-> Week 4: Exploring the World of Generative AI

    Day 19: Started a course on Generative AI - super fascinating stuff! Learned about the fundamentals of AI, machine learning, deep learning, and even cool applications like chatGPT.
    Day 20: Completed the Generative AI course! Learned about LLMs (Large Language Models) - basically AI that can generate text like a human - prompt engineering, and how Generative AI is being used in the real world. I even built a chatbot using OpenAI and some cool libraries like Streamlit, Langchain, and PyPDF2!
    

   -> Technologies and Concepts During Course:

   - AI (Artificial Intelligence): Broad field of computer science focused on creating intelligent machines.
   - Machine Learning (ML): Subfield of AI where machines learn from data without explicit programming.
   - Deep Learning (DL): Subfield of ML using artificial neural networks with multiple layers to process complex data.
   - Natural Language Processing (NLP): Subfield of AI focused on the interaction between computers and human language.
   - MySQL: A popular open-source relational database management system.
   - Generative AI: A type of AI that can create new data, like text, images, or code.
   - Large Language Models (LLMs): AI models trained on massive amounts of text data to generate human-quality text.
   - Prompt Engineering: The art of crafting specific prompts or questions to guide the output of an LLM.
   - Embeddings: A technique for representing text or other data in a lower-dimensional space while preserving relationships between data points.
   - Fine-tuning: Adapting a pre-trained model for a specific task.
   - Streamlit: A Python library for building data apps.
   - Langchain: A library for working with large language models.
   - PyPDF2: A Python library for working with PDF files.
   - Responsible AI: The ethical development and use of AI.


    Day 21: Began implementing the chatbot as shown in the Gen AI course.
    Day 22: Created a chatbot capable of answering questions based on a provided document.
    Day 23: Extended the chatbot's functionality to handle and respond to questions from a specific PDF document.
    Day 24: Installed Flutter and acquired basic knowledge to integrate the chatbot into a Flutter application.

-> Week 5: created a chatbot for Human Resource Policy

    Day 25: start watching how's the flutter works and grep the live experience by coding it.
    Day 26: Completed UI for my chatbot application.
    Day 27: Linked my python backend to my flutter frontend UI.
    Day 28: Test the application on local server.
    Day 29: Given a better look to the application.
    Day 30: Create ngrok account for test my application on android.


-> Week 6: Deployment of the application

    Day 31: Started creating the chatbot for my slyce project.
    Day 32: Create a UI for the chatbot using Flutter.
    Day 33: Trying to connect Flutter directly through the python by giving python dependancies from pub.dev
    Day 34: Still Working on it Unfortunately its not happen Let's see for the future works it is work or not.
    Day 35: wrapup the Application.
    
